<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="xuzichao03@gmail.com"><title>IOS 音频一览 · LorisBlog</title><meta name="description" content="做嘉宾聊天室这个需求的时候接触了音频和视频上的开发，嘉宾可以发布视频和语音以及文字信息出来，观众则可以文字回复，这样主持人嘉宾和观众就形成一个良性的互动过程。在这里把相关整理一下，望给大家带来一些认识。

一、了解音频声音作为信息的一种媒介载体必不可少，在移动端体现为各类语音交流以及音乐等，开发中使"><meta name="keywords" content="开发者,程序猿,编程,用户体验,产品,IOS,UI,UE,视觉设计,FE,Golang"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">LorisBlog</a></h3><div class="description"><p>「About Life and Code」</p></div></div></div><ul class="social-links"></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/archives">文章</a></li><li><a href="/about">关于</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="/images/logo.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>IOS 音频一览</a></h3></div><div class="post-content"><blockquote>
<p>做嘉宾聊天室这个需求的时候接触了音频和视频上的开发，嘉宾可以发布视频和语音以及文字信息出来，观众则可以文字回复，这样主持人嘉宾和观众就形成一个良性的互动过程。在这里把相关整理一下，望给大家带来一些认识。</p>
</blockquote>
<h1 id="一、了解音频"><a href="#一、了解音频" class="headerlink" title="一、了解音频"></a>一、了解音频</h1><p>声音作为信息的一种媒介载体必不可少，在移动端体现为各类语音交流以及音乐等，开发中使用的音频文件通过声音采样、量化、编码几步从而成为人耳可听的声音，频率为20HZ到20KHZ，奈奎斯特的理论表明采样率高于最高频2倍时候，是可以将数字信号还原为原来的模拟信号的，所以通常我们在网上看到的音频文件的采样率为44.1KHZ。</p>
<p>采样后通过量化的脉冲编码调制，我们得到了PCM类型的数据，比如在使用AVAudioRecorder采样的时候可以看到采样类型kAudioFormatLinearPCM。但是这样拿到的数据是很大的，不便于使用和传输，通过对人类不敏感的声音区域进行过滤压缩，就有了MP3、AAC、OGG、WMA等数据格式类型，这些都是有损压缩。</p>
<p>码率代码了压缩质量，比如MP3常用码率有128kbit/s、160kbit/s、320kbit/s等等，越高代表着声音音质越好。MP3中的数据有ID3和音频数据组成，ID3用于存储歌名、演唱者、专辑、音轨等我们可以常见的信息。</p>
<h1 id="二、IOS对音频的操作"><a href="#二、IOS对音频的操作" class="headerlink" title="二、IOS对音频的操作"></a>二、IOS对音频的操作</h1><p>开发实际上是为了解决需求，需求对应的是使用场景，开发的方式很多，不同的使用场景可以使用的方法不同。IOS开发提供了以下几种常用方式供我们解决对应的音频需求。</p>
<ul>
<li>System Sound Services </li>
<li>AVFoundation 框架</li>
<li>Media Player 框架</li>
</ul>
<h2 id="2-1-System-Sound-Services"><a href="#2-1-System-Sound-Services" class="headerlink" title="2.1  System Sound Services"></a>2.1  System Sound Services</h2><h4 id="2-1-1-场景特点"><a href="#2-1-1-场景特点" class="headerlink" title="2.1.1 场景特点"></a>2.1.1 场景特点</h4><p>最底层也是简单的声音播放服务，此方法是适合播放提示警告类型的短小的声音</p>
<h4 id="2-1-2-存在限制"><a href="#2-1-2-存在限制" class="headerlink" title="2.1.2 存在限制"></a>2.1.2 存在限制</h4><ul>
<li>声音长度要小于 30 秒</li>
<li>In linear PCM 或者 IMA4 (IMA/ADPCM) 格式的</li>
<li>打包成 .caf, .aif, 或者 .wav 的文件</li>
<li>不能控制播放的进度</li>
<li>调用方法后立即播放声音</li>
<li>没有循环播放和立体声控制：</li>
</ul>
<h4 id="2-1-3-使用方式"><a href="#2-1-3-使用方式" class="headerlink" title="2.1.3 使用方式"></a>2.1.3 使用方式</h4><p>调用 AudioServicesCreateSystemSoundID(CFURLRef inFileURL,SystemSoundID *outSystemSoundID) 该函数的第一个参数代表音频文件的URL（可通过NSURL转换成CFURLRef），第二个参数代表注册音频文件的SystemSoundID。</p>
<p>调用AudioServicesAddSystemSoundCompletion()函数为制定SystemSoundID注册Callback函数。有了 CallBack 函数我们可以解决不少问题，比如可以克服 System Sound Services 本身不支持循环播放的问题。</p>
<p>调用AudioServicePlaySystemSound函数或者AudioServicePlayAlertSound（调用系统振动功能）。</p>
<pre><code>- (void)viewDidLoad
{
[super viewDidLoad];

// 1. 定义要播放的音频文件的URL
NSURL *voiceURL = [[NSBundle mainBundle]URLForResource:@&quot;CleanDidFinish&quot; withExtension:@&quot;aiff&quot;];

// 2. 注册音频文件（第一个参数是音频文件的URL 第二个参数是音频文件的SystemSoundID）
AudioServicesCreateSystemSoundID((__bridge CFURLRef)(voiceURL),&amp;ditaVoice);

// 3. 为crash播放完成绑定回调函数AudioServicesAddSystemSoundCompletion(ditaVoice,NULL,NULL,(void*)completionCallback,NULL);

// 4. 播放 ditaVoice 注册的音频 并控制手机震动
AudioServicesPlayAlertSound(ditaVoice);

//    AudioServicesPlaySystemSound(ditaVoice);
//    AudioServicesPlaySystemSound(kSystemSoundID_Vibrate); // 控制手机振动

}
</code></pre><h2 id="2-2-AVFoundation-框架"><a href="#2-2-AVFoundation-框架" class="headerlink" title="2.2  AVFoundation 框架"></a>2.2  AVFoundation 框架</h2><h4 id="2-2-1-场景特点"><a href="#2-2-1-场景特点" class="headerlink" title="2.2.1 场景特点"></a>2.2.1 场景特点</h4><p>如果播放较大的音频或者要对音频有精确的控制，则System Sound Service可能就很难满足实际需求了，通常这种情况会选择使用AVFoundation，它可以满足我们通常意义上的绝大部分的场景需求，包括音乐的交互、声音的制作等等，根据自己的业务需求实现自定义的定制化。如果你只是想实现音频的播放或者录制，没有其他需求，AVFoundation会很好的满足你，它的接口使用简单、不用关心其中的细节。</p>
<h4 id="2-2-2-关键点和API浏览"><a href="#2-2-2-关键点和API浏览" class="headerlink" title="2.2.2 关键点和API浏览"></a>2.2.2 关键点和API浏览</h4><blockquote>
<p>Background Modes</p>
</blockquote>
<p>打开后台模式的音乐播放，或者在info.plist文件中添加Required Background Modes键，其值是App plays audio or streams audio/video using AirPlay</p>
<blockquote>
<p>AVAudioSession</p>
</blockquote>
<p>用于 iOS 系统中协调应用程序之间的音频播放的 API 的。例如，当有电话打进来时，音频的播放就会被暂停；在用户启动电影时，音乐的播放就会停止。我们需要使用这些 API 来确保一个应用程序能够正确响应并处理这类事件。</p>
<blockquote>
<p>AVAudioPlayer</p>
</blockquote>
<p>这个高层级的 API 为你提供一个简单的接口，用来播放本地或者内存中的音频。这是一个无界面的音频播放器 (也就是说没有提供 UI 元素)，使用起来也很直接简单。它不适用于网络音频流或者低延迟的实时音频播放。如果这些问题都不需要担心，那么 AVAudioPlayer 可能就是正确的选择。音频播放器的 API 也为我们带来了一些额外的功能，比如循环播放、获取音频的音量强度等等。</p>
<blockquote>
<p>AVAudioRecorder</p>
</blockquote>
<p>作为与 AVAudioPlayer 相对应的 API，AVAudioRecorder 是将音频录制为文件的最简单的方法。除了用一个音量计接受音量的峰值和平均值以外，这个 API 简单粗暴，但要是你的使用场景很简单的话，这可能恰恰就是你想要的方法。</p>
<blockquote>
<p>AVPlayer</p>
</blockquote>
<p>AVPlayer 与上面提到的 API 相比，提供了更多的灵活性和可控性。它基于 AVPlayerItem 和 AVAsset，为你提供了颗粒度更细的权限来获取资源，比如选择指定的音轨。它还通过 AVQueuePlayer 子类支持播放列表，而且你可以控制这些资源是否能够通过 AirPlay 发送。</p>
<h4 id="与-AVAudioPlayer-最主要的区别是，AVPlayer-对来自网络的流媒体资源的-“开箱即用”-支持。这增加了处理播放状态的复杂性，但是你可以使用-KVO-来观测所有的状态参数来解决这个问题。"><a href="#与-AVAudioPlayer-最主要的区别是，AVPlayer-对来自网络的流媒体资源的-“开箱即用”-支持。这增加了处理播放状态的复杂性，但是你可以使用-KVO-来观测所有的状态参数来解决这个问题。" class="headerlink" title="与 AVAudioPlayer 最主要的区别是，AVPlayer 对来自网络的流媒体资源的 “开箱即用” 支持。这增加了处理播放状态的复杂性，但是你可以使用 KVO 来观测所有的状态参数来解决这个问题。"></a>与 AVAudioPlayer 最主要的区别是，AVPlayer 对来自网络的流媒体资源的 “开箱即用” 支持。这增加了处理播放状态的复杂性，但是你可以使用 KVO 来观测所有的状态参数来解决这个问题。</h4><blockquote>
<p> AVAudioEngine</p>
</blockquote>
<p>AVAudioEngine 是播放和录制的 Objective-C 接口。它提供了以前需要深入到 Audio Toolbox 框架的 C API 才能做的控制 (例如一些实时音频任务)。该音频引擎 API 对底层的 API 建立了优秀的接口。如果你不得不处理底层的问题，你仍然可以使用 Audio Toolbox 框架。</p>
<p>这个 API 的基本概念是建立一个音频的节点图，从源节点 (播放器和麦克风) 以及过处理 (overprocessing) 节点 (混音器和效果器) 到目标节点 (硬件输出)。每一个节点都具有一定数量的输入和输出总线，同时这些总线也有良好定义的数据格式。这种结构使得它非常的灵活和强大。而且它集成了音频单元 (audio unit)。</p>
<h2 id="2-3-Media-Player-框架"><a href="#2-3-Media-Player-框架" class="headerlink" title="2.3 Media Player 框架"></a>2.3 Media Player 框架</h2><h4 id="2-3-1-场景特点"><a href="#2-3-1-场景特点" class="headerlink" title="2.3.1 场景特点"></a>2.3.1 场景特点</h4><p>众所周知音乐是iOS的重要组成播放，无论是iPod、iTouch、iPhone还是iPad都可以在iTunes购买音乐或添加本地音乐到音乐库中同步到你的iOS设备。在MediaPlayer.frameowork中有一个MPMusicPlayerController用于播放音乐库中的音乐。Media Player 框架是 iOS 平台上一个用于音频和视频播放的高层级接口，它包含了一个你可以在应用中直接使用的默认的用户界面。你可以使用它来播放用户在 iPod 库中的项目，或者播放本地文件以及网络流。这个框架也包括了查找用户媒体库中内容的 API，同时还可以配置像是在锁屏界面或者控制中心里的音频控件。</p>
<h4 id="2-3-2-使用方式"><a href="#2-3-2-使用方式" class="headerlink" title="2.3.2 使用方式"></a>2.3.2 使用方式</h4><p>使用MPMusicPlayerController实例化对象来播放内置音乐库的媒体文件，有以下两种类方法来实例化对象：</p>
<p>MPMusicPlayerController *playController = [MPMusicPlayerController systemMusicPlayer]; </p>
<p>说明：播放内置媒体库项目取代用户目前播放状态（如果是用网易云音乐或QQQ音乐在播放歌曲）</p>
<p>MPMusicPlayerController *playController = [MPMusicPlayerController applicationMusicPlayer]; </p>
<p>说明：播放该应用内的歌曲，不影响本机自带音乐播放器的状态。</p>
<ul>
<li><p>判断有没有正在播放的媒体</p>
<pre><code>[MPMusicPlayerController indexOfNowPlayingItem] == NSNotFound
</code></pre></li>
<li><p>创建媒体队列</p>
<pre><code>[MPMediaQuery songsQuery];
[MPMusicPlayerController setQueueWithQuery:nil];
</code></pre></li>
<li><p>获取媒体曲目的信息</p>
</li>
</ul>
<pre><code>MPMediaItem *currentItem = ....
NSString *artist = [currentItem valueForProperty:MPMediaItemPropertyArtist];
NSString *songName = [currentItem valueForProperty:MPMediaItemPropertyTitle];
</code></pre><ul>
<li><p>监听媒体通知</p>
<pre><code>NSNotificationCenter *notificationCenter = [NSNotificationCenter defaultCenter];[notificationCenter addObserver:self
              selector:@selector()
                       name:MPMusicPlayerControllerNowPlayingItemDidChangeNotification
                     object:nil];
</code></pre></li>
</ul>
<h2 id="2-4-更多音频方案"><a href="#2-4-更多音频方案" class="headerlink" title="2.4 更多音频方案"></a>2.4 更多音频方案</h2><p>CoreAudio的接口层次：</p>
<p><img src="/assets/images/api.png" alt=""></p>
<h4 id="2-4-1-OpenAL"><a href="#2-4-1-OpenAL" class="headerlink" title="2.4.1 OpenAL"></a>2.4.1 OpenAL</h4><p>OpenAL 是一个跨平台的 API。它提供了位置 (3D) 和低延迟的音频服务。它主要用于跨平台游戏的开发。它有意地模仿了 OpenGL 中 API 的风格。</p>
<h4 id="2-4-2-Audio-Unit-框架"><a href="#2-4-2-Audio-Unit-框架" class="headerlink" title="2.4.2  Audio Unit 框架"></a>2.4.2  Audio Unit 框架</h4><p>Audio Unit 框架是一个底层的 API；所有 iOS 中的音频技术都构建在 Audio Unit 这个框架之上。音频单元是用来加工音频数据的插件。一个音频单元链叫做音频处理图。</p>
<p>如果你需要非常低的延迟 (如 VoIP 或合成乐器)、回声消除、混音或者音调均衡的话，你可能需要直接使用音频单元，或者自己写一个音频单元。但是其中的大部分工作可以使用 AVAudioEngine 的 API 来完成。如果你不得不写自己的音频单元的话，你可以将它们与 AVAudioUnit 节点一起集成在 AVAudioEngine 处理图中。</p>
<h4 id="2-4-2-AudioToolBox-框架"><a href="#2-4-2-AudioToolBox-框架" class="headerlink" title="2.4.2 AudioToolBox 框架"></a>2.4.2 AudioToolBox 框架</h4><p>通过AudioToolbox框架，可以将短声音注册到system sound服务上，被注册到system sound服务上的声音称之为 system sounds。<br>前面常用的System Sound Services 就来自这里的框架。</p>
<p>它必须满足下面几个条件：</p>
<p>(1).播放的时间不能超过30秒</p>
<p>(2).数据必须是 PCM或者IMA4流格式</p>
<p>(3).必须被打包成下面三个格式之一：Core Audio Format (.caf), Waveform audio (.wav), 或者 Audio Interchange File (.aiff)</p>
<p>(4）声音文件必须放到设备的本地文件夹下面。通过AudioServicesCreateSystemSoundID方法注册这个声音文件.</p>
<h4 id="2-4-3-CoreMIDI-和-CoreAudioKit-框架"><a href="#2-4-3-CoreMIDI-和-CoreAudioKit-框架" class="headerlink" title="2.4.3 CoreMIDI 和 CoreAudioKit 框架"></a>2.4.3 CoreMIDI 和 CoreAudioKit 框架</h4><p>在 iOS 上，Core MIDI 和 CoreAudioKit 可以被用来使应用程序表现为 MIDI 设备。在 OS X 上，Music Sequencing 服务提供了基于 MIDI 的控制和对音乐数据访问的权限。Core MIDI 服务为服务器和驱动程序提供了支持。</p>
<h4 id="2-4-4-QTKit-和-QuickTime-框架"><a href="#2-4-4-QTKit-和-QuickTime-框架" class="headerlink" title="2.4.4 QTKit 和 QuickTime 框架"></a>2.4.4 QTKit 和 QuickTime 框架</h4><p>现在已经过时了，它们不应该被用在以后的开发中。我们应该使用 AVFoundation (和 AVKit) 来代替它们</p>
<h1 id="三、今日头条嘉宾聊天室音频实践"><a href="#三、今日头条嘉宾聊天室音频实践" class="headerlink" title="三、今日头条嘉宾聊天室音频实践"></a>三、今日头条嘉宾聊天室音频实践</h1><h2 id="3-1-背景"><a href="#3-1-背景" class="headerlink" title="3.1 背景"></a>3.1 背景</h2><p>嘉宾聊天室是头条16年初新起的一个项目服务，目标是为了引进明星嘉宾访谈类型和体育赛事线上直播的节目，丰富头条在直播领域的内容，聊天室一期是属于图文、语音、短视频直播，后期发展可成为视频线上直播。表现形式与网易直播频道类似，但更丰富。</p>
<h2 id="3-2-IOS端实现"><a href="#3-2-IOS端实现" class="headerlink" title="3.2 IOS端实现"></a>3.2 IOS端实现</h2><p>根据聊天室在语音和视频的需求，使用系统自带实现的MPMoviePlayerController没法符合自定义的需求，包括功能与交互设计，并且MPMoviePlayerController已经不被苹果官方提倡，将要通过AVPlayer方案代替。需求本身的属于基本的语音和视频沟通，只对音频视频的录制与播放，符合音质画质要求和大小要求，因此采用AVFoundation框架即可，能够满足聊天室的需求。主要使用了AVPlayer、AVAudioRecorder、AVAudioSession等主要的类。</p>
<p>实现类有如下等：</p>
<pre><code>#import &quot;TTAudioRecorder.h&quot;
#import &quot;TTAudioPlayer.h&quot;
#import &quot;TTLiveCameraViewController.h&quot;
#import &quot;TTUploadVideoAudioManager.h&quot;
#import &quot;TTLiveAudioManager.h&quot;
</code></pre><h2 id="3-3-问题与解决"><a href="#3-3-问题与解决" class="headerlink" title="3.3 问题与解决"></a>3.3 问题与解决</h2><p>整个需求的实现过程还算顺利，按照API说明理解即可，提两三点说明下都会遇见哪一类的问题。</p>
<h4 id="3-1-音频格式为AMR"><a href="#3-1-音频格式为AMR" class="headerlink" title="3.1 音频格式为AMR"></a>3.1 音频格式为AMR</h4><p>AVPlayer来播放视频音频都相当强大，但是它也存在着一些不可回避的问题，那就是目前IOS已经不再支持AMR格式的播放。</p>
<pre><code>AMR format is no longer supported by Apple (since iOS 4.3)
</code></pre><p>与安卓同步开发的时候对接确立的通用的格式为AMR，因为AAC文件在网络传输下载播放的时候显得很大，AMR相对来说会好很多，同时我们也看了微博和微信的实现都是使用AMR，鉴于头条的用户量和使用体验，于是就采用了同样的方式。安卓可以很好支持AMR，在iOS平台上需要进行WAV和AMR之间的转换，好在libopencore可以解决这个事。网络上有好些所谓的相互转化的库，仔细看了下，都没有脱离这个core本身。</p>
<p>libopencore库：</p>
<pre><code>interf_dec.h 
interf_enc.h
dec_if.h
if_rom.h
libopencore-amrnb.a
libopencore-amrwb.a
</code></pre><p>主要方法：</p>
<pre><code>EncodeWAVEFileToAMRFile 、 DecodeAMRFileToWAVEFile
</code></pre><p>封装一个mannager：        </p>
<pre><code>#import &lt;Foundation/Foundation.h&gt;

@interface VoiceConverter : NSObject

+ (int)amrToWav:(NSString*)_amrPath wavSavePath:(NSString*)_savePath;

+ (int)wavToAmr:(NSString*)_wavPath amrSavePath:(NSString*)_savePath;

@end
</code></pre><h4 id="3-2-WAV转化AMR声音变形："><a href="#3-2-WAV转化AMR声音变形：" class="headerlink" title="3.2 WAV转化AMR声音变形："></a>3.2 WAV转化AMR声音变形：</h4><p>录制WAV格式本地正常播放，转化为AMR后，把AMR格式文件在电脑端播放，声音严重变形，无法识别，再转化会WAV,，手机还是无法识别。<br>原因与解决<br>声音格式转化采用的是”amrFileCodec.h”，它对转化的音频输入源是有格式要求的，要求转化的采样率为标准的8k，如果录制的音频频率采用高频率44.1K的话就会出现变形，我想这里的设定依据来自于amr格式的采样率通常为8K。通过AVAudioRecorder把采样率设置为8K后，可以正常互相转化。</p>
<blockquote>
<p>AMR维基百科：</p>
<p>采样率 8 kHz/13-bit (160 采样点每20ms)，滤波后只保留 200-3400 Hz 范围内的信号.</p>
<p>编码器使用8个位速：12.2、10.2、7.95、7.40、6.70、5.90、5.15和4.75 kbit/s.</p>
</blockquote>
<pre><code>NSMutableDictionary *settings=[NSMutableDictionary dictionary];
[settings setObject:@(kAudioFormatLinearPCM) forKey:AVFormatIDKey];
[settings setObject:@(8000) forKey:AVSampleRateKey]; //必须和amr文件解码参数保持一致
[settings setObject:@(1) forKey:AVNumberOfChannelsKey];
[settings setObject:@(16) forKey:AVLinearPCMBitDepthKey];
[settings setObject:@(NO) forKey:AVLinearPCMIsFloatKey];
[settings setValue:@(NO) forKey:AVLinearPCMIsNonInterleaved];
[settings setValue:@(NO) forKey:AVLinearPCMIsBigEndianKey];
[settings setValue:@(AVAudioQualityHigh) forKey:AVEncoderAudioQualityKey];
AVAudioRecorder *recorderTemp = [[AVAudioRecorder alloc] initWithURL:fileUrl settings:settings error:nil];
</code></pre><h4 id="3-3-AVPlayer的准备状态"><a href="#3-3-AVPlayer的准备状态" class="headerlink" title="3.3 AVPlayer的准备状态"></a>3.3 AVPlayer的准备状态</h4><p>当AVPlayer的status变为AVPlayerStatusReadyToPlay后，依旧可能无法开始播放？</p>
<p>AVPlayerStatusReadyToPlay属性只是表明了AVPlayer已经成功的载入了AVPlayerItem，并且准备好，但是实际的是否能播放时由AVPlayerItem的status到达AVPlayerItemStatusReadyToPlay的时候，才能开始正常播放的。<br>如果我们的App使用CPU过多，I/O读写过多时，有可能导致直接无法播放，我们再调用play或者seekToTime:方法都无法正常播放，尤其是视频。所以我们需要做一个真正播放状态准备好的判断，也可以通过KVO去监听AVPlayerItem的status。</p>
<pre><code>//播放器是否准备好
if (self.videoPrePlayer.status == AVPlayerStatusReadyToPlay) 
{
    //视频是否加载成功
    if(self.videoPrePlayer.currentItem.status == AVPlayerItemStatusFailed){   
        return;
    }
        [self.videoPrePlayer play];

}
</code></pre><h1 id="四、小结"><a href="#四、小结" class="headerlink" title="四、小结"></a>四、小结</h1><p>文章主要对音频的使用范畴做了概括，并列举了两三实践点，其他的并没有做详细的论述，因为本篇比较偏向音频知识的介绍，知道用什么工具框架后再具体解决就好。比如如何播放流畅的网络音频，如何实现音频的快放与慢放等具体问题，此次需求内容并不复杂没有涉及，他们都可以通过 AudioToolBox框架实现。</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2015-03-15</span><i class="fa fa-tag"></i><a href="/tags/开发/" title="开发" class="tag">开发 </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://zichao.me/2015/03/15/IOS 音频一览/,LorisBlog,IOS 音频一览,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2015/07/22/自动布局 Masonry/" title="自动布局 Masonry" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2015/01/05/RunLoop是如何跑起来的/" title="RunLoop是如何跑起来的" class="btn">下一篇</a></li></ul></div><a id="comments"></a><div id="youyan_thread" class="post"></div><script>(function() {

    var YYDiv = document.createElement('div');
    YYDiv.id = 'uyan_frame';
    document.getElementById('youyan_thread').appendChild(YYDiv);

    var YYScript = document.createElement('script');
    YYScript.type = 'text/javascript';
    YYScript.async = true;
    YYScript.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//v2.uyan.cc/code/uyan.js?uid=' + '2143349';
    YYScript.charset = 'UTF-8';
    document.getElementById('youyan_thread').appendChild(YYScript);

})();</script></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>